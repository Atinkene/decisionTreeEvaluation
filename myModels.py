# -*- coding: utf-8 -*-
"""myModels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EWK498XpRz0cbt2S1poifq_ETZ-v_MUe

# Importer les bibliothèques nécessaires
"""

# !pip install pandas joblib scikit-learn
# !pip install optuna
import os
import numpy as np
import pandas as pd
import joblib
import optuna
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier

# """# Monter Google Drive"""

# from google.colab import drive
# drive.mount('/content/drive')

"""# Charger les données"""

file_path = "./Copie de DatasetmalwareExtrait.csv"
data = pd.read_csv(file_path, sep=",")
X = data.drop(columns=["legitimate"])
y = data["legitimate"]

"""# Division des données en données d'entraînement et de test"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print("Entraînement :", X_train.shape)
print("Test :", X_test.shape)

"""# Modèle Decision Tree"""

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_predictions)
print(f"Précision du modèle Decision Tree : {dt_accuracy:.2f}")
joblib.dump(dt_model, 'dt_model.py')

"""# Modèle SVM"""

svm_model = SVC()
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)
print(f"Précision du modèle SVM : {svm_accuracy:.2f}")
joblib.dump(svm_model, 'svm_model.py')

"""# Modèle K-Neighbors"""

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_predictions = knn_model.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_predictions)
print(f"Précision du modèle K-Neighbors : {knn_accuracy:.2f}")
joblib.dump(knn_model, 'knn_model.py')

"""# Résumé des performances"""

print("\nRésumé des performances des modèles :")
print(f"Précision de l'Arbre de Décision : {dt_accuracy:.2f}")
print(f"Précision du SVM : {svm_accuracy:.2f}")
print(f"Précision des K-Neighbors : {knn_accuracy:.2f}")

"""# Hyperparamètres avec Optuna"""

def objectif(essai):
    profondeur_max = essai.suggest_int('profondeur_max', 10, 50)
    division_min = essai.suggest_int('division_min', 2, 20)
    feuille_min = essai.suggest_int('feuille_min', 1, 10)
    critere = essai.suggest_categorical('critere', ['gini', 'entropy'])

    modele = DecisionTreeClassifier(
        max_depth=profondeur_max,
        min_samples_split=division_min,
        min_samples_leaf=feuille_min,
        criterion=critere
    )
    score = cross_val_score(modele, X_train, y_train, cv=3, scoring='accuracy').mean()
    return score

etude = optuna.create_study(direction='maximize')
etude.optimize(objectif, n_trials=50)
print(f"Meilleurs hyperparamètres Optuna : {etude.best_params}")
print(f"Meilleur score : {etude.best_value:.2f}")

"""# Random Forest initial et optimisation"""

clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred_initial = clf.predict(X_test)

precision_initial = precision_score(y_test, y_pred_initial, average='weighted')
recall_initial = recall_score(y_test, y_pred_initial, average='weighted')
f1_initial = f1_score(y_test, y_pred_initial, average='weighted')
conf_matrix_initial = confusion_matrix(y_test, y_pred_initial)

param_dist = {
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'criterion': ['gini', 'entropy']
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=10,
    cv=3,
    random_state=42,
    n_jobs=-1
)
random_search.fit(X_train, y_train)

best_model = random_search.best_estimator_
y_pred_optimized = best_model.predict(X_test)

precision_optimized = precision_score(y_test, y_pred_optimized, average='weighted')
recall_optimized = recall_score(y_test, y_pred_optimized, average='weighted')
f1_optimized = f1_score(y_test, y_pred_optimized, average='weighted')
conf_matrix_optimized = confusion_matrix(y_test, y_pred_optimized)

results = pd.DataFrame({
    'Metric': ['Precision', 'Recall', 'F1 Score',],
    'Without Optimization': [precision_initial, recall_initial, f1_initial],
    'With Optimization': [precision_optimized, recall_optimized, f1_optimized]
})

print("Comparaison des métriques :\n", results)
print("\nMatrice de confusion sans optimisation :\n", conf_matrix_initial)
print("\nMatrice de confusion avec optimisation :\n", conf_matrix_optimized)

"""# Optimisation avec Optuna pour RandomForest"""

def objective(trial):
    max_depth = trial.suggest_int('max_depth', 10, 50, step=10)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])

    model = RandomForestClassifier(
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        criterion=criterion,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return f1_score(y_test, y_pred, average='weighted')

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

print("\nMeilleurs hyperparamètres avec Optuna :\n", study.best_params)